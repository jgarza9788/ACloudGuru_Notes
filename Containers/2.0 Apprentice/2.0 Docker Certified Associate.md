

**Exam Details:**

*   **Exam Title:** Docker Certified Associate
*   **Exam Code:** DCA
*   **Exam Duration:** 90 minutes
*   **Number of Questions:** Approximately 55-60 multiple-choice questions
*   **Passing Score:** 70%
*   **Exam Cost:** The cost may vary depending on your location and the testing center.

**Topics Covered:** The Docker Certified Associate exam covers a wide range of topics related to Docker and containerization. These topics include:

1.  **Docker Fundamentals:**
    
    *   Understanding containerization concepts.
    *   Docker architecture, components, and services.
    *   Basic Docker commands and operations.
2.  **Docker Installation and Configuration:**
    
    *   Installing and configuring Docker on various platforms.
    *   Managing Docker configurations and settings.
3.  **Container Management:**
    
    *   Creating, running, and managing Docker containers.
    *   Working with container images and Docker Hub.
4.  **Docker Networking:**
    
    *   Configuring and managing Docker networking.
    *   Understanding container communication and connectivity.
5.  **Docker Storage and Volumes:**
    
    *   Managing data persistence with Docker volumes.
    *   Understanding storage drivers and options.
6.  **Docker Compose:**
    
    *   Defining and managing multi-container applications using Docker Compose.
    *   Compose file structure and syntax.
7.  **Docker Security:**
    
    *   Securing Docker containers and images.
    *   Implementing security best practices.
    *   User authentication and authorization.
8.  **Docker Orchestration:**
    
    *   Introduction to Docker Swarm and Kubernetes.
    *   Deploying and managing containerized applications in an orchestrated environment.
9.  **Docker Logging and Monitoring:**
    
    *   Collecting and analyzing container logs.
    *   Implementing monitoring and alerting solutions.
10.  **Troubleshooting and Maintenance:**
    
    *   Diagnosing and troubleshooting common Docker issues.
    *   Performing regular maintenance tasks.

**Preparation:** To prepare for the Docker Certified Associate exam, consider the following steps:

1.  **Study the Official Documentation:** Review the Docker documentation thoroughly as it covers all the essential topics.
    
2.  **Hands-On Experience:** Gain practical experience by working with Docker containers and images.
    
3.  **Training Courses:** Consider taking Docker training courses or online tutorials.
    
4.  **Practice Exams:** Use practice exams and sample questions to assess your readiness.
    
5.  **Study Guides:** Explore study guides and books related to Docker certification.
    
6.  **Docker Playground:** Utilize Docker playgrounds or sandboxes to experiment with Docker in a risk-free environment.
    
7.  **Join the Community:** Participate in Docker communities and forums to ask questions and learn from others.
    

**Conclusion:** The Docker Certified Associate certification demonstrates your proficiency in Docker containerization technology. It's a valuable credential for professionals involved in container-based application development, deployment, and management. Prepare thoroughly, gain hands-on experience, and you'll be well-equipped to pass the DCA exam and further your career in containerization and DevOps.


---

# Course Features and Tools.txt

* Cloud Playground: Accessible through the "Playground" button, allows you to spin up cloud servers for hands-on learning.
* Hands-On Labs: Offers self-contained environments with temporary servers and step-by-step instructions.
* Course Scheduler: Helps create a study schedule for the course content.
* Flash Cards: Useful for memorization, especially important for the text-based multiple-choice Docker Certified Associate exam.
* Practice Exam: A multiple-choice exam similar to the Docker Certified Associate exam for preparation.
* Community Tab: Offers ways to connect with instructors and other students, ask questions, and provide assistance during the course.

---

# Docker Editions

* Docker Editions: Docker offers two editions, Community Edition (CE) and Enterprise Edition (EE).
* Docker CE: The free and open-source version of Docker Engine.
* Core Functionality: Docker CE provides core container functionality, including running containers, Docker Swarm, networking, and security.
* Feature Parity: Docker CE and EE have the same core features and receive updates on the same schedule.
* Emphasis: This section focuses on Docker Community Edition, covering installation, configuration, Docker Swarm, and basic Docker functionality.

---



* Docker CE Installation: This lesson demonstrates the installation of Docker Community Edition (CE) on a CentOS server.
* Server Setup: A CentOS 7 server is created in the cloud playground for installation.
* Required Packages: Device-mapper-persistent-data and lvm2 packages are installed as prerequisites.
* Adding Docker Repo: Docker CE repository is added using the yum-config-manager command.
* Docker Installation: Docker CE is installed with a specific version (18.09.5), along with docker-ce-cli and containerd.io.
* Docker Service Management: The Docker service is started and enabled using systemctl to ensure it runs on server startup.
* User Permissions: The cloud_user is added to the docker group to grant Docker command access.
* Testing Docker: Docker is verified by running a simple docker run command to execute a container.
* Successful Installation: The container runs successfully, confirming the Docker installation on CentOS.
* Next Steps: The lesson concludes with a preview of installing Docker CE on an Ubuntu environment in the next lesson.


# Installing Docker CE (CentOS)

To Install Docker CE on CentOS, we will
need to do the following:

* Provision a Server
    * Image: CentOS 7
    * Size: Small
* Install Required Packages: Install some
required packages (yum-utils, device-
mapper-persistent-data, and lvm2).
* Add the Docker Repo
* Install Docker and Containerd packages
* Start and Enable the Docker Service
* Configure cloud_user to be Able to Use
Docker: Add cloud user to the docker
group, then log out and back in.
* Run a Container to Test the
Installation: Test the installation by
running the hello-world image.

# Installing Docker CE (Ubuntu)

To Install Docker CE on Ubuntu, we will
need to do the following:

* Provision a Server
    * Image: Ubuntu 18.04 Bionic Beaver LTS
Size: Small
* Install Required Packages
* Add the Docker GPG Key and Repo
* Install Docker and Containerd packages
* Configure cloud_user to be Able to Use
Docker: Add cloud user to the docker
group, then log out and back in.
* Run a Container to Test the
Installation: Test the installation by
running the hello-world image.

---

# Selecting a Storage Driver

Storage drivers provide a pluggable
framework for managing the temporary,
internal storage of a container's writable
layer.
Docker supports a variety of storage
drivers. The best storage driver to use
depends on your environment and your
storage needs.
overlay2: File-based storage. Default for
Ubuntu and CentOS 8+.
devicemapper: Block storage, more
efficient for doing lots of writes. Default for
CentOS 7 and earlier.
You can find out what storage driver is
currently configured with docker info

# Selecting a Storage Driver

Docker automatically selects a default
storage driver that is compatible with your
environment.
However, in some cases you may want to
override the default to use a different
driver.
There are two ways to do this:
* Set the --storage-driver flag when
starting Docker (in your system unit file
for example).
"storage-driver" value in
* Set the
/etc/docker/daemon.json.

```bash

docker info
# shows the storage driver

# edit this file 
sudo vi /user/lib/systemd/system/docker.service
# change the storage driver
# in vi
--storage-driver devicemapper
# exit vi

# reload docker
sudo systemctl daemon-reload
sudo systemctl restart docker
docker info

# ^ undo this change above

# /////////////////////////////////

sudo vi /etc/docker/daemon.json
# in vi
{
    "system-driver": "devicemapper"
}
#exit vi

# reload docker
sudo systemctl daemon-reload
sudo systemctl restart docker
docker info

```

# running Containers

* Running Containers: This lesson focuses on running containers using the docker run command.
* Docker Run Basics: The basic structure of the docker run command is introduced, emphasizing the required elements: docker, run, and a reference to an image.
* Image Tags: Images can have tags, representing versions or variants. Without specifying a tag, Docker automatically chooses the latest version.
* Running Specific Commands: Demonstrates running containers with specific commands and arguments, allowing customization of container behavior.
* Detached Mode (-d): Explains the -d flag, used to run containers in detached mode, allowing them to run in the background and releasing the command prompt.
* Naming Containers (--name): Shows how to assign custom names to containers using the --name flag, making it easier to manage and reference them.
* Container Restart Policies (--restart): Discusses different restart policies (no, on-failure, always, unless-stopped) for controlling container restart behavior in various scenarios.
* Port Mapping (-p): Demonstrates using the -p flag to publish container ports, making them accessible from the host system or the network.
* Automatic Container Removal (--rm): Explains the --rm flag, which automatically deletes containers upon stopping, preventing lingering containers from taking up disk space.
* Memory Limits (--memory and --memory-reservation): Covers setting memory limits for containers using --memory and --memory-reservation flags.
* Listing Containers (docker ps and docker ps -a): Shows how to list currently running containers with docker ps and all containers (including stopped ones) with docker ps -a.
* Stopping and Starting Containers (docker container stop and docker container start): Illustrates stopping and starting containers using docker container stop and docker container start.
* Deleting Containers (docker container rm): Explains how to delete containers with docker container rm, ensuring that containers are first stopped if necessary.
* Managing Containers: Summarizes key container management commands and concepts learned in the lesson.


```bash

docker run hello-world
# runs a container in docker

docker run nginx:1.15.11
# runs nginx with tag

docker run busybox echo hello-world!
# starts a busybox with an hello-world command

docker run -d nginx 
# -d detatched mode (runs in background)
docker ps

docker container stop <id>


docker run -d --name nginx --restart always nginx 
# restart in an endless loop

docker run -d --name nginx --restart unless-stopped -p 8080:80 nginx 
# -p publish the port
# 8080 <- host part on the server
# 80 <- port for inside the container

# --rm : remove the container when done

#--memory 500M 
# memory limit

#--memory-reservation 256M
# memory reservation (should be less then the memory limit, if server is under a lot of load)

```


# Running a Container
Some commonly-used options:
* -d: Run container in detached mode.
The docker run command will exit
immediately and the container will run
in the background.

* --name: A container is assigned a
random name by default, but you can
give it a more descriptive name with this
flag.

* --restart: Specify when the container
should be automatically restarted.
    * no (default): Never restart the container.
    * on-failure: Only if the container fails (exits with a non-zero exit code).
    * always: Always restart the container whether it succeeds or fails. Also starts the container automatically on daemon startup.
    * unless-stopped: Always restart the container whether it succeeds or fails, and on daemon startup, unless the container was manually stopped.


[docker run documentation](https://docs.docker.com/engine/reference/run/)


```bash
docker ps
# list all active containers

curl localhost:8080

docker ps -a
# shows all the containers, even past containers

docker container stop <name or id>
docker container start <name of id>
docker container rm <name of id>
# stop the container before remove



```

## Upgrading Docker Engine: 
This lesson covers the process of upgrading the Docker Engine.
Initial State: The instructor is currently using Docker Community Edition version 18.09.5 on an Ubuntu Server.

## Downgrading: 
The instructor explains that they will first demonstrate downgrading Docker before proceeding with the upgrade.

## Steps to Downgrade:
Stop Docker: sudo systemctl stop docker.

## Remove Docker packages: 
sudo apt-get remove docker-ce docker-ce-cli.
Install an earlier version (downgrade): sudo apt-get install -y docker-ce=18.09.4 docker-ce-cli=18.09.4.

## Upgrading Docker: 
The instructor shows that upgrading Docker is simpler and does not require stopping Docker or removing packages.

## Steps to Upgrade:
Install the new version: sudo apt-get install -y docker-ce=18.09.5 docker-ce-cli=18.09.5.

## Confirmation: 
The instructor verifies the Docker version using the docker version command.

## Summary: 
The lesson demonstrates both the downgrade and upgrade processes for Docker, emphasizing that upgrading involves installing a newer version of Docker packages.


```bash

sudo systemctl stop docker

sudo apt-get remove -y docker-ce docker-ce-cli

sudo apt-get update

sudo apt-get install -y docker-ce=5:18.09.4~3-0~ubuntu-bionic docker-ce-cli=5:18.09.4~3-0~ubuntu-bionic

docker version

```


# Configuring Logging Drivers
Logging drivers are a pluggable framework
for accessing log data from services and
containers in Docker. Docker supports a
variety of logging drivers.

Configure the default logging driver by
setting log-driver and log-opts
in
/etc/docker/daemon.json

You override the default logging driver and
options for a container with the --log-driver
and --log-opt flags when using docker run.


Logging Drivers in Docker: 
This lesson covers the concept of logging drivers in Docker, which allows customization of log data handling for containers and services.

Variety of Logging Drivers: 
Docker supports various logging drivers, and users can choose from them based on their needs.

Default Logging Driver: 
Docker has a system-wide default logging driver, which can be overridden for individual containers.

No Need to Configure Default: 
Docker has a built-in logging driver configuration, so users don't need to configure it if the default is acceptable.

Setting Default Logging Driver: 
To set the default logging driver, edit the daemon.json file (usually located at /etc/docker/daemon.json) and specify the desired logging driver using the log-driver key.

Additional Logging Driver Configuration: 
Users can also configure additional options for logging drivers using the log-opts key.

Example: Demonstrates setting the default logging driver to json-file with a max size of 15 megabytes.

Restart Docker: After editing the daemon.json file, restart Docker for changes to take effect.

Overriding Logging Driver for Containers: Users can override the default logging driver for individual containers using the --log-driver flag with docker run.

Example: Runs an Nginx container with the syslog logging driver.
Overriding Logging Driver Options: Users can also override logging driver options for containers using the --log-opt flag with docker run.

Example: Overrides the max-size option for a container.
Customized Logging Configuration: Allows tailoring logging configurations to the specific needs of individual containers.

Summary: The lesson covers setting the system-wide default logging driver, overriding it for containers, and customizing logging configurations for specific container requirements.

```bash

docker info | grep Logging
# no swap

sudo vi /ect/docker/daemon.json
{
    "log-driver": "json-file", // syslog,  
    "log-opts": {
        "max-size": "15m"
    }
}
#exit vi

#restart docker
sudo systemctl restart docker
docker info


# ////

docker run --log-driver syslog nginx
# runs with the syslog log driver
# ctrl + c -- to stop it

docker run --log-driver json-file --log-opt max-size=50m nginx
# change the max-size of the log file

```

[docker logging](https://docs.docker.com/config/containers/logging/configure/)


* **Introduction to Docker Swarm**: This lesson introduces Docker Swarm and its purpose.
* **Cluster Setup**: The instructor will guide viewers on setting up a Docker Swarm cluster in upcoming lessons.
* **Distributed Cluster**: Docker Swarm enables the creation of distributed clusters of Docker machines for container management.
* **Useful Features**: Docker Swarm offers features like orchestration, high availability, and scaling.
* **Course Focus**: The course will delve deeper into implementing these features in subsequent lessons.
* **Preparation**: Viewers are encouraged to set up a Docker Swarm cluster in the Linux Academy cloud playground.
* **Cluster Configuration**: Specifies a 3-node cluster with 1 manager and 2 worker nodes.
* **Server Requirements**: Requires using Ubuntu 18.04 Bionic Beaver LTS, with a medium size for the manager and small sizes for the workers.
* **Resource Management**: Suggests deleting unnecessary servers from the playground to ensure adequate resources.
* **Follow Along**: Encourages viewers planning to follow along to set up their servers in the playground.

# Docker Swarm
Docker includes a feature called swarm
mode, which allows you to build a
distributed cluster of docker machines to
run your containers.

Docker swarm provides many useful
features, and can help facilitate
orchestration, high-availability, and scaling.
Let's build a swarm!
Provision some servers:

1. Manager
* Image: Ubuntu 18.04 Bionic Beaver LTS
* Size: Medium

2. Workers
* Image: Ubuntu 18.04 Bionic Beaver LTS
* Size: Small


# Configuring a Swarm Manager

* **Configuring Docker Swarm Manager**:
  * Introduction to the lesson about configuring a Docker Swarm manager.
  * Explanation of the role of a swarm manager in controlling and managing the cluster.
  * Overview of the upcoming setup with a swarm manager and worker nodes.
* **Steps to Configure Swarm Manager**:
  * Two main steps: Installing Docker CE and initializing a new swarm cluster.
  * Details of the Docker CE installation process.
  * Adding the user to the Docker group for command execution.
* **Initializing Swarm Cluster**:
  * Explanation that initializing the cluster on a server automatically makes it the swarm manager.
  * Demonstration of initializing the cluster using "docker swarm init."
  * Importance of specifying the "--advertise-address" with the private IP address in Linux Academy cloud playground.
* **Confirmation of Swarm Configuration**:
  * Verification of swarm status using "docker info."
  * Checking the list of nodes in the swarm using "docker node ls."
  * Confirmation that the swarm manager is active and ready.
* **Next Steps**:
  * Teaser for the next lesson, which will cover setting up swarm worker nodes.


```bash
sudo apt-get update
sudo apt-get -y install \
apt-transport-https \
ca-certificates \
curl \
gnupg-agent \
software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo apt-key fingerprint 0EBFCD88
sudo add-apt-repository \
"deb [arch=amd64] https://download.docker.com/linux/ubuntu \
$(lsb_release -cs) \
stable"
sudo apt-get update
sudo apt-get install -y docker-ce=5:18.09.5~3-0~ubuntu-bionic docker-ce-cli=5:18.09.5~3-0~ubuntu-bionic conta
sudo usermod -a -G docker cloud_user

docker swarm init --advertise-addr <swarm manager private IP>

docker info

docker node ls

```

# configuring swarm nodes (worker nodes)


* **Configuring Swarm Nodes**:
  * Introduction to the lesson about configuring swarm nodes.
  * Explanation of the roles of swarm manager and worker nodes.
  * Clarification that worker nodes perform the actual work.
* **Steps to Configure Swarm Nodes**:
  * Installation of Docker CE on worker nodes.
  * Demonstration of the installation process on two worker nodes in parallel.
  * Addition of "cloud_user" to the docker group and logging out and in for changes to take effect.
* **Joining Worker Nodes to the Swarm**:
  * Generation of a join command with a join token on the swarm manager using "docker swarm join-token worker."
  * Copying and pasting the generated join command to worker nodes for joining the cluster.
* **Confirmation of Swarm Configuration**:
  * Verification of swarm status on worker nodes using "docker info."
  * Observation that worker nodes have joined as workers.
  * Viewing swarm information on the manager node.
  * Checking the list of swarm nodes using "docker node ls."
* **Summary and Future Topics**:
  * Successful initialization of a 3-node cluster with 1 manager and 2 worker nodes.
  * Teaser for the next lesson on backup and restoration with Docker swarm.
  * Mention of upcoming sections focusing on the practical use of Docker swarm.



```bash

sudo apt-get update
sudo apt-get -y install \
apt-transport-https \
ca-certificates \
curl \
gnupg-agent \
software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo apt-key fingerprint 0EBFCD88
sudo add-apt-repository \
"deb [arch=amd64] https://download.docker.com/linux/ubuntu \
$(lsb_release -cs) \
stable"
sudo apt-get update
sudo apt-get install -y docker-ce=5:18.09.5~3-0~ubuntu-bionic docker-ce-cli=5:18.09.5~3-0~ubuntu-bionic conta
sudo usermod -a -G docker cloud_user

docker swarm join-token worker

docker swarm join --token <token> <swarm manager private IP>:2377

docker node ls

```



# back and restore with docker swarm

* **Backup and Restore for Docker Swarm**:
  * Overview of the lesson on backing up and restoring Docker swarm.
  * Introduction to the process on a swarm manager.
* **Backup Procedure**:
  * Stopping the Docker service temporarily for backup purposes.
  * Using `sudo tar` to create a backup archive (`backup.tar.gz`) of the `/var/lib/docker/swarm` directory.
  * Restarting Docker service after completing the backup.
  * Mention of automating backups through shell scripts or cron jobs.
* **Restoration Procedure**:
  * Clearing the contents of `/var/lib/docker/swarm` to ensure a clean directory.
  * Extracting the backup contents from `backup.tar.gz` into `/var/lib/docker/swarm`.
  * Restarting the Docker service.
  * Checking the status with `docker node ls` to confirm successful restoration.
* **Summary**:
  * Emphasis on the simplicity of backup and restoration for Docker swarm by managing the data in `/var/lib/docker/swarm` directory.
  * Conclusion of the lesson.


```bash
sudo systemctl stop docker
# stops docker

sudo tar -zvcf backup.tar.gz /var/lib/docker/swarm
# tar/zip up everythihng
ls
# check for the tar.gz

sudo systemctl start docker
# restart the docker

## -- restore

sudo systsemctl stop docker
sudo rm -rf /var/lib/docker/swarm/*
sudo tar -zxvf backup.tar.gz -C /var/lib/docker/swarm/
sudo systsemctl start docker
docker node ls


```

# Namespaces and Control Groups in Docker

* **Namespaces and Control Groups in Docker**:
  * Explanation of key concepts for the Docker Certified Associate Exam.
* **Namespaces**:
  * Overview of how namespaces isolate processes from resources.
  * Mention that namespaces limit resource visibility and usage in a transparent manner.
  * Docker's use of namespaces for container isolation.
  * List of some Docker namespaces and their purposes: pid, net, ipc, mnt, and uts.
  * Special configuration required for the user namespace, allowing a container process to run as root while mapped to an unprivileged user on the host.
* **Control Groups (cgroups)**:
  * Explanation that cgroups limit resource usage rather than visibility.
  * Docker's use of cgroups to enforce resource usage rules.
* **Exam Preparation**:
  * General understanding of namespaces and cgroups and their roles in Docker.
  * Awareness of common Docker namespaces and the unique nature of the user namespace.
* **Conclusion**:
  * End of the lesson and preparation for the next one.


# Namespaces
Namespaces are a Linux technology that
allows processes to be isolated in terms of
the resources that they see. They can be
used to prevent different processes from
interfering or interacting with one another.

Docker uses namespaces to isolate
containers. This technology allows
containers to operate independently and
securely.

Docker uses namespaces such as the
following to isolate resources for
containers:

* pid: Process isolation
* net: Network interfaces
* ipc: Inter-process communication 
* mnt: Filesystem mounts
* uts: Kernel and version identifiers
* user namespaces: Requires special configuration. Allows container processes to run as root inside the container while mapping that user to an unprivileged user on the host.






Installing and Configuring the Docker Engine
============================================

Introduction
------------

Docker CE is a great way to get started using the Docker engine. It is free and open-source, but provides a high-quality container runtime. This lab will help you practice the steps involved in installing and configuring the Docker Engine. You will practice installing Docker CE and configuring a logging driver. This lab will help provide you with some basic insight into how the Docker Engine is installed and configured on systems in the real world.

### Instructions

Your company is ready to start using Docker on some of their servers. In order to get started, they want you to set up and configure Docker CE on a server that has already been set up. You will need to make sure that the server meets the following specifications:

*   Docker CE is installed and running on the server.
*   Use the latest Docker CE version.
*   The user `cloud_user` has permission to run Docker commands.
*   The default logging driver is set to `syslog`.

If you get stuck, feel free to check out the solution video, or the detailed instructions under each objective. Good luck!

**Note:** _When copying and pasting code into Vim from the lab guide, first enter `:set paste` (and then `i` to enter insert mode) to avoid adding unnecessary spaces and hashes. To save and quit the file, press_ **Escape** _followed by `:wq`. To exit the file_ **without** _saving, press_ **Escape** _followed by `:q!`._

Solution
--------

Log in to the server using the credentials provided:

`ssh cloud_user@<PUBLIC_IP_ADDRESS>`
ssh cloud_user@34.200.253.41
34.200.253.41
OsVGwp6(


### Install Docker CE on the Server

1.  First, make sure old versions of Docker are not present on the system:
    
    `sudo apt-get remove docker docker-engine docker.io containerd runc`
    
2.  Then, update the apt package index:
    
    `sudo apt update -y`
    
3.  Install the packages needed for apt to use a repository over HTTPS:
    
    `sudo apt install -y ca-certificates curl gnupg`
    
4.  Add the official Docker GPG key:
    
```
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
```

    
5.  Change the `docker.gpg` file permissions:
    
    `sudo chmod a+r /etc/apt/keyrings/docker.gpg`
    
6.  Set up the repository:
    
    `
echo \
"deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
"$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    `
    
7.  Update the list of available packages:
    
    `sudo apt -y update`
    
8.  Install Docker packages:
    
    `sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin`
    
    > **Note:** The package installation may take a few minutes.
    
9.  Verify that your installation is working:
    
    `sudo docker run hello-world`
    

### Give `cloud_user` Access to Run Docker Commands

1.  Make sure the `docker` group was created:
    
    `sudo groupadd docker`
    
2.  Add your user to the `docker` group:
    
    `sudo usermod -aG docker $USER`
    
3.  Either log out and back in as the `cloud_user`, or run the following command:
    
    `newgrp docker`
    
4.  Once you are logged back in, or have executed the mentioned command, verify the access for the `cloud_user`:
    
    `docker run hello-world`
    

### Set the Default Logging Driver to `syslog`

1.  Edit `daemon.json`:
    
    `sudo vi /etc/docker/daemon.json`
    
2.  Add configuration to `daemon.json` to set the default logging driver:
    
    `{ "log-driver": "syslog" }`
    
3.  Restart Docker:
    
    `sudo systemctl restart docker`
    
4.  Verify that the logging driver was set properly:
    
    `docker info | grep Logging`
    
5.  This command should return a line that says:
    
    `Logging Driver: syslog`
    
6.  Configure Docker to start on boot:
    
    `sudo systemctl enable docker.service`
    
7.  Configure the containerd service to start on boot:
    
    `sudo systemctl enable containerd.service`
    

Conclusion
----------

Congratulations, you've completed this hands-on lab!


------------------------------------------------------------------------

Building a Docker Swarm
=======================

Introduction
------------

Docker swarm allows you to quickly move beyond simply using Docker to run containers. With swarm, you can easily set up a cluster of Docker servers capable of providing useful orchestration features. This lab will allow you to become familiar with the process of setting up a simple swarm cluster on a set of servers. You will configure a swarm master and two worker nodes, forming a working swarm cluster.

### Instructions

Your company is ready to move forward with using Docker to run their applications. However, they have some complex container apps that can take advantage of the cluster management and orchestration features of Docker swarm. You have been asked to stand up a simple Docker swarm cluster to be used for some initial testing. A set of servers has already been provisioned for this purpose. The swarm cluster should meet the following criteria:

*   One Swarm manager.
*   Two worker nodes.
*   All nodes should use Docker CE version `5:18.09.5~3-0~ubuntu-bionic`.
*   Both worker nodes should be joined to the cluster.
*   `cloud_user` should be able to run docker commands on all three servers.

If you get stuck, feel free to check out the solution video, or the detailed instructions under each objective. Good luck!

Solution
--------

Begin by logging in to the lab server using the credentials provided on the hands-on lab page:

`ssh cloud_user@PUBLIC_IP_ADDRESS`

# Swarm Manager
ssh cloud_user@3.236.177.66
Y*Y4L#cg

# Swarm worker1
ssh cloud_user@44.193.229.119
Y*Y4L#cg

# Swarm worker2
ssh cloud_user@34.201.8.158
Y*Y4L#cg



### Install Docker CE on all three nodes

1.  On all three servers, install Docker CE.

`sudo apt-get update`

`
sudo apt-get -y install apt-transport-https ca-certificates 
curl gnupg-agent software-properties-common
`

`curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -`

`
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
`

`sudo apt-get update`

`sudo apt-get install -y docker-ce=5:18.09.5~3-0~ubuntu-bionic docker-ce-cli=5:18.09.5~3-0~ubuntu-bionic containerd.io`

1.  Add cloud\_user to the Docker group so that you can run docker commands as cloud\_user.

`sudo usermod -a -G docker cloud_user`

Log out each server, then log back in.

1.  You can verify the installation on each server like so:

`docker version`

### Configure the swarm manager

1.  On the swarm manager server, initialize the swarm. Be sure to replace `<swarm manager private IP>` in this command with the actual Private IP of the swarm manager (NOT the public IP).

`docker swarm init --advertise-addr <swarm manager private IP>`
docker swarm init --advertise-addr 3.236.177.66

### Join the worker nodes to the cluster

1.  On the swarm manager, get a join command with a token:

`docker swarm join-token worker`

This should provide a command that begins `docker swarm join ...`. Copy that command and run it on both worker servers.

1.  Go back to the swarm manager and list the nodes.

`docker node ls`

Verify that you can see all three servers listed (including the manager). All three should have a status of `READY`. Once all three servers are ready, you have built your own Docker swarm cluster!

Conclusion
----------

Congratulations, you've completed this hands-on lab!


# Introduction to Docker Images

# Docker Images

Docker image is a file containing the
code and components needed to run
software in a container.

Containers and images use a layered file
system. Each layer contains only the
differences from the previous layer.

The image consists of one or more read-
only layers, while the container adds one
addition writable layer.


* Docker images are files containing all the code and components needed to run software in a container.
* Docker images consist of layers, each containing differences from the previous layer.
* Images use a layered file system to share common layers, reducing storage footprint and speeding up image transfer.
* The writeable container layer is added when a container is run and captures changes to the file system.
* Shared layers enable multiple containers to use the same base layers, optimizing resource usage.
* The layered file system makes image transfer faster and image building more efficient.
* Use the `docker image history` command to examine the layers in an image.


```md
[writable container layer] <- writeable

[web application] <- unwriteable
[python] <- unwriteable
[base ubuntu OS image] <- unwriteable
```


```bash
docker run nginx:1.15.8
# runs the container
# images are layers

docker image history <imagename>
# shows the history of the layers


```

# the components of a dockerfile

* Dockerfile is used to build Docker images, containing a set of instructions called directives.
* `FROM` directive specifies the base image.
* `ENV` directive sets environment variables for the Dockerfile.
* `RUN` directive executes a command and creates a new file system layer.
* `CMD` directive provides a default command when running a container.
* Images can be built from a Dockerfile using `docker build`.
* Docker caches layers to speed up subsequent builds if no changes occur.
* Containers can be run from the custom image using `docker run`.
* Additional image customization will be covered in the next lesson.


```bash
mkdir custom-nginx
cd custom-nginx/
vi index.html
# ~in dockerfile~
# hello world
#exit dockerfile
vi Dockerfile 
# ~in dockerfile~
#Simple ngin image
FROM ubuntu:bionic # our OS

ENV NGINX_VERSION 1.14.0-0ubuntu1.11 # variables

RUN apt-get update && apt-get install -y curl
RUN apt-get update && apt-get install -y nginx:$NGINX_VERSION

CMD ["nginx","-g","daemon off;"]

# exit dockerfile

docker build -t custom-nginx .
# build this into an image

docker fun -d -p 8080:80 custom-nginx
curl localhost:8080
# shows nginx welcome page

ls
#lists

cat index.html
#hello world!

```



# more dockerfile directives

* `EXPOSE` directive documents the ports intended to be listened on by the image.
* `WORKDIR` sets the current working directory within the container during build and runtime.
* Multiple `WORKDIR` directives can be used, either absolute or relative to the previous one.
* `COPY` and `ADD` directives move files into the container; `ADD` has additional features like downloading files from URLs or extracting archives.
* `STOPSIGNAL` specifies the signal used to terminate the container process.
* `HEALTHCHECK` allows customization of container health checks, useful for more complex health monitoring.
* Docker images can be built and tested using these directives in the Dockerfile.


```bash
mkdir custom-nginx
cd custom-nginx/
vi index.html
# ~in dockerfile~
# hello world
#exit dockerfile
vi Dockerfile 
# ~in dockerfile~
#Simple ngin image
FROM ubuntu:bionic # our OS

ENV NGINX_VERSION 1.14.0-0ubuntu1.11 # variables

RUN apt-get update && apt-get install -y curl
RUN apt-get update && apt-get install -y nginx:$NGINX_VERSION

WORKDIR /var #sets main directory when building and running
WORKDIR www #sets the child dir as a main dir
WORKDIR /etc #sets a new main directory

COPY index.html ./ #allows us to move files into this container
# this will move the file into the last WORKDIR

ADD index.html ./
# similer to copy
# but
# can copy from html
# can exstract archive

EXPOSE 80 # documents the port 

CMD ["nginx","-g","daemon off;"]

STOPSIGNAL SIGTERM 

HELATHCHECK CMD curl localhost:80 #custom health check

# exit dockerfile

docker build -t custom-nginx .
# buidl the docker image

docker run -d -p 8080:80 custom-nginx
curl localhost:8080
# get "hello world"

docker ps

docker container rm -f <container id>
```


# Building Efficient Images

* Building efficient Docker images is important for minimizing image size and resource usage.
* To build efficient images, consider the following tips:
  * Place less likely to change items on lower-level layers to benefit from caching.
  * Avoid creating unnecessary layers by bundling commands together.
  * Exclude unnecessary files, packages, or resources from the image.
* Multi-stage builds are a technique to create more efficient images by using multiple `FROM` directives in a Dockerfile.
* In multi-stage builds, each `FROM` directive starts from scratch, allowing you to copy only necessary files from previous stages.
* This technique significantly reduces the final image size, as unnecessary files and dependencies are excluded.
* Multi-stage builds are particularly useful when a build process requires additional tools or dependencies that aren't needed in the final image.


```bash

cd ~/
mkdir efficient
mkdir inefficient
cd inefficient

vi helloworld.go
##
package main
import "fmt"
func main() {
fmt.Println("hello world")
}
##

vi Dockerfile
##
FROM golang:1.12.4
WORKDIR /helloworld
COPY helloworld.go .
RUN GOOS=linux go build -a -installsuffix cgo -o helloworld .
CMD ["./helloworld"]
##

docker build -t inefficient .
docker run inefficient
docker image ls

cd ~/efficient
cp ../inefficient/helloworld.go ./
cp ../inefficient/Dockerfile ./

vi Dockerfile
FROM golang:1.12.4 AS compiler
WORKDIR /helloworld
COPY helloworld.go .
RUN GOOS=linux go build -a -installsuffix cgo -o helloworld .
FROM alpine:3.9.3
WORKDIR /root
COPY --from=compiler /helloworld/helloworld .
CMD ["./helloworld"]

docker build -t efficient .
docker run efficient
docker image ls
```

By using a multi-stage build, you ensure that only the required artifacts (the compiled executable) are included in the final image, leading to a smaller image size, reduced resource consumption, and faster image distribution. This approach is especially valuable when you have build dependencies that are not needed in the runtime environment.


# Managing Images

* `docker image pull` or `docker pull`: Downloads Docker images from Docker Hub if they don't already exist locally.
* `docker image ls` or `docker images`: Lists the Docker images on your system, and `-a` includes intermediate images.
* `docker image inspect`: Provides detailed image metadata in JSON format, with customizable output using `--format`.
* `docker image rm` or `docker rmi`: Removes Docker images, with caution when using `-f` (force) to avoid image deletion conflicts.
* Dangling Images: Images without tags or container references can become "dangling" after tag removal.
* `docker image prune`: Removes dangling images, freeing up disk space.

These are fundamental commands for Docker image management. For advanced operations, consult the official Docker documentation.

```bash

docker image ls
# lists the docker images

docker image ls -a
# all images - even the base images


docker image inspect <imagename> 
# shows details on the image

docker image inspect <imagename> --format "{{.Architechure}}"
docker image inspect <imagename> --format "{{.Architechure} {.Os}}"
# shows details on the image with custom format

docker image rm <imagename>
#delete image

docker rmi <imagename>
#delete image

docker run -d --name nginx nginx:1.14.0
docker image rm nginx:1.14.0
# fails since a container is using it

docker image rm -f nginx:1.14.0
# deletes the image anyways

docker image prune
# deletes all unused images



```


# Flattening a Docker Image to a Single Layer

* Flattening an image involves converting a multi-layered image into a single layer, typically for performance optimization.
* Docker does not officially support flattening images, but it can be done for specific use cases.
* The process involves running a container from the image, exporting the container's file system into an archive, and then importing the archive as a new image.
* Steps:
  1. Create a Dockerfile for your image.
  2. Build the image and tag it.
  3. Run a container from the image.
  4. Export the container's file system to a TAR archive using `docker export`.
  5. Import the TAR archive as a new image using `docker import`.
* The resulting image will have a single layer, but it won't necessarily save space as compared to the original image.
* Flattening images is not a common practice and should be done only when necessary for specific requirements.



```bash


cd ~/ 
mkdir alpine-hello
cd alphine-hello
vi Dockerfile

##
FROM alpine:3.9.3
RUN echo "HELLO, WORLD" > message.txt
CMD cat message.txt
##

docker build -t nonflat
docker image history nonflat

docker run -d --name flat_container nonflat
docker export flat_container > flat.tar

ls
# .tar file here


cat flat.tar | docker import - flat:latest

docker image ls

docker image history flat 
docker image history nonflat


```

# Docker registries


* Docker registries are central repositories for storing and distributing Docker images.
* Docker Hub is a public Docker registry available on the internet, where you can find and share Docker images.
* Private Docker registries offer more control and security for storing and distributing images within a private network.
* Docker provides free and open-source registry software that you can run to create your own private registry.
* Docker Enterprise offers Docker Trusted Registry (DTR), which includes enterprise-level features for managing Docker images.
* To run a basic Docker registry, you can use the `docker run` command with the `registry:2` image, exposing port 5000.
* You can configure the Docker registry using environment variables to customize settings like log levels.
* To enable authentication and secure communication, you can generate an htpasswd file for authentication and create self-signed TLS certificates.
* Docker registries are typically accessed via HTTPS, and self-signed certificates can be used for securing the registry.
* Testing the registry's functionality by making requests to it can help ensure it's set up correctly.
* In the next lesson, you'll learn how to connect to a Docker registry, push and pull images, and effectively use your private registry.



```bash


docker run -d -p 5000 --restart=always --name registry registry:2
# runs a docker registry 

docker logs registry 
# log level is info

docker container strop registry && docker container rm -v registry
# stops the container

docker run -d -p 5000 --restart=always --name registry -e REGISTRY_LOG_LEVEL=debug registry:2
# -e for environment variable

docker logs registry

mkdir ~/registry
cd ~/registry/
mkdir auth
docker run --entrypoint htpasswd registry:2 -Bbn testuser password > auth/htpasswd
ls auth/htpasswd
cat auth/htpasswd

mkdir certs
openssl reg \
-newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \
-x509 -days 365 -out certs/domain.crt
# except default until common name
# common name should be the webaddress 

docker run -d -p 443:443 --restart=always --name registry \
-v /home/cloud_user/registry/certs:/certs \
-v /home/cloud_user/registry/auth:/auth \
-e REGISTRY_HTTP_ADDR=0.0.0.0:443 \
-e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
-e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
-e REGISTRY_AUTH=htpasswd \
-e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
-e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
registry:2


curl -k https://localhist:443

```

# using docker registry

* Docker registries are repositories for storing and distributing Docker images.
* Docker provides commands like `docker pull` to download images from a registry and `docker search` to search for images on Docker Hub.
* Authentication may be required to access private registries or private images on Docker Hub. Use `docker login` to authenticate.
* To interact with private registries with self-signed certificates, you can disable certificate verification (not recommended for security) or configure Docker to trust the certificate.
* To configure Docker to trust a self-signed certificate, copy the certificate to the local machine and place it in the directory `/etc/docker/certs.d/<registry_hostname>/`.
* You can push images to a private registry by tagging the image with the registry's hostname and then using `docker push`.
* To pull images from a private registry, ensure the image is not locally available and use `docker pull`.
* Deleting an image with `docker image rm` removes the tag, and you may need to delete the underlying image to ensure a fresh pull from the registry.
* Understanding both insecure methods and secure certificate-based authentication is essential for Docker Certified Associate exam preparation.




```bash

docker search ubuntu
# searches docker hub

docker login
# logs into dockerhub

docker login wboyd4c.mylabserver.com
# logs into private registry 
## fails

# turn off cert verification
sudo vi /etc/docker/daemon.json

##
{
  "insecure-registry": ["wboyd4c.mylabserver.com"]
}
##

sudo systemctl restart docker 
docker login wboyd4c.mylabserver.com

## undo ^ that

sudo mkdir -p /etc/docker/certs.d/wboyd4c.mylabserver.com
sudo scp could_user@wboyd4c.mylabserver.com:/home/cloud_user/registry/certs/domain.crt /etc/docker/certs.d/wboyd4c.mylabserver.com

ls /etc/docker/certs.d/wboyd4c.mylabserver.com/domain.crt

docker login wboyd4c.mylabserver.com
# worked!

docker pull ubuntu 
# pull the ubuntu image

docker tag ubuntu wboyd4c.mylabserver.com/ubuntu
docker push ubuntu wboyd4c.mylabserver.com/ubuntu

docker image rm wboyd4c.mylabserver.com/ubuntu
docker image rm ubuntu 

docker pull wboyd4c.mylabserver.com/ubuntu
# pulls from the private registry 

```

Creating Your Own Docker Image
==============================

Introduction
------------

Docker Hub provides many useful, pre-made images which you can use for a variety of applications. However, if you want to use Docker in the real world, you will likely be required to design and build your own Docker images, either to customize existing images or to run your own software.

In this lab, you will have the opportunity to work with Docker images by designing your own image to a set of specifications using a Dockerfile. You will then be able to run a container using your image to verify that it works.

Solution
--------

1.  Begin by logging in to the lab server using the credentials provided on the hands-on lab page:

`ssh cloud_user@PUBLIC_IP_ADDRESS`
ssh cloud_user@54.83.231.10
P50C*miI

### Create a Dockerfile to define the image and build it

1.  Change to the project directory and create a Dockerfile.

`cd ~/fruit-list 
vi Dockerfile`

1.  Build a Dockerfile that meets the provided specifications.

`
FROM nginx:1.15.8

ADD static/fruit.json /usr/share/nginx/html/fruit.json
ADD nginx.conf /etc/nginx/nginx.conf

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
`

1.  Build the image.

`docker build -t fruit-list:1.0.0 .`

### Run a container with the image in detached mode and verify that it works

1.  Run a container in detached mode using the newly-created image.

`docker run --name fruit-list -d -p 8080:80 fruit-list:1.0.0`

1.  Make a request to the container and verify that you get some JSON with a list of fruits.

`curl localhost:8080`

Conclusion
----------

Congratulations — you've completed this hands-on lab!


########################################



Building a Private Docker Registry
==================================

Docker registries provide a powerful way to manage and distribute your Docker images. Docker offers a free registry at Docker Hub, but in many scenarios, you may want greater control of your images, not to mention that it is not free to have more than one private repository on Docker Hub. Fortunately, you can build and manage your own private registries, allowing you full control over where your images are housed and how they can be accessed.

In this lab, you will have the opportunity to work with a private registry. You will build your own private registry, and you will have a chance to practice some advanced setup to ensure that your private registry is secure. After completing this lab, you will know how to set up a simple but secure private Docker registry.

### Solution

Begin by logging in to the lab servers using the credentials provided on the hands-on lab page:

`ssh cloud_user@PUBLIC_IP_ADDRESS`

registry server
ssh cloud_user@35.175.171.102
tEdM#)5s

ws server
ssh cloud_user@34.229.74.1
tEdM#)5s


It is a good idea to log in to both servers at the same tab in separate tabs in your terminal application.

### Set up the private registry

1.  In the **Registry** server, create an `htpasswd` file containing the login credentials for the initial account.
    
    `mkdir -p ~/registry/auth`
    
    `
    mkdir -p ~/registry/auth
    docker run --entrypoint htpasswd \
    registry:2.7.0 -Bbn docker d0ck3rrU73z > ~/registry/auth/htpasswd
    `
    
2.  Create a directory to hold the certs for the registry server
    
    `mkdir -p ~/registry/certs`
    
3.  Create a self-signed certificate for the registry. **NOTE:** For the _Common Name_ field, enter the hostname of the registry server, which is `ip-10-0-1-101`. For the other prompts, just hit **enter** to accept the default value.
    
    `
openssl req \
-newkey rsa:4096 -nodes -sha256 -keyout ~/registry/certs/domain.key \
-x509 -days 365 -out ~/registry/certs/domain.crt
    `

ip-10-0-1-101

4.  Create a container to run the registry.
    
    `
docker run -d -p 443:443 --restart=always --name registry \
-v /home/cloud_user/registry/certs:/certs \
-v /home/cloud_user/registry/auth:/auth \
-e REGISTRY_HTTP_ADDR=0.0.0.0:443 \
-e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
-e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
-e REGISTRY_AUTH=htpasswd \
-e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
-e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
registry:2.7.0
    `
    
5.  Once the registry starts up, verify that it is responsive. It's OK if this command returns nothing, just make sure it does not fail.
    
    `curl -k https://localhost:443`
    

### Test the registry from the Docker workstation server

1.  Get the public hostname from the registry server. It should be `ip-10-0-1-101`.
    
    `echo $HOSTNAME`
    
2.  On the **Workstation** server, add the registry's public self-signed certificate to `/etc/docker/certs.d`. The `scp` command is copying the file from the registry server to the workstation. The password is the normal `cloud_user` password provided by the lab.
    
    > **Note**: The following steps should be completed from the **Workstation** server.
    
    `sudo mkdir -p /etc/docker/certs.d/ip-10-0-1-101:443`
    
    `sudo scp cloud_user@ip-10-0-1-101:/home/cloud_user/registry/certs/domain.crt /etc/docker/certs.d/ip-10-0-1-101:443`
    
3.  Log in to the private registry from the workstation. The credentials should be username `docker` and password `d0ck3rrU73z`.
    
    `docker login ip-10-0-1-101:443`
    
4.  Test the registry by pushing an image to it. You can pull any image from Docker hub and tag it appropriately to push it to the registry as a test image.
    
    `docker pull ubuntu`
    
    `docker tag ubuntu ip-10-0-1-101:443/test-image:1`
    
    `docker push ip-10-0-1-101:443/test-image:1`
    
5.  Verify image pulling by deleting the image locally and re-pulling it from the private repository.
    
    `docker image rm ip-10-0-1-101:443/test-image:1`
    
    `docker image rm ubuntu:latest`
    
    `docker pull ip-10-0-1-101:443/test-image:1`
    

Conclusion
----------

Congratulations — you've completed this hands-on lab!


# Orchestration 

* Docker swarm uses autolock to protect encryption keys for sensitive data, including raft logs and TLS communication between swarm nodes.
* Autolock is turned off by default, which means encryption keys are stored unencrypted on swarm managers' disks.
* When autolock is enabled, it requires an unlock key to be provided every time Docker restarts on a manager, enhancing security but requiring manual intervention.
* To enable autolock, use the `docker swarm update --autolock=true` command.
* Store the unlock key securely as it's essential for unlocking the swarm after manager restarts.
* You can also enable autolock during swarm initialization using `docker swarm init --autolock=true`.
* Autolock can be disabled using `docker swarm update --autolock=false`.
* To unlock a swarm manager, use `docker swarm unlock` and provide the unlock key when prompted.
* Use `docker swarm unlock-key` to retrieve the unlock key if it's lost but a manager is still unlocked.
* Rotate the unlock key with `docker swarm unlock-key --rotate` for added security. Keep the old key temporarily in case of propagation delays.
* With autolock off, you won't need to unlock the swarm after manager restarts.
* Understanding autolock and its management is important for securing your Docker swarm cluster.


```bash 

docker swarm update --autolock=true
# save the swarm key ... to unlock the logs
# or
docker swarm update --autolock=false
# to keep autolock off

docker node ls
# lists the nodes

sudo systemctl restart docker
# this will lock the swarm 

docker node ls
# error - the swarm is locked 

docker swarm unlock
# enter the swarm key 


docker node ls 
# this worked 

docker swarm unlock-key 
# gives you the unlocked key 

docker swarm unlock-key --rotate
# gives you a new key

docker swarm update --autolock=false
# turn off autolock 
# swarm doesn't need to be unlocked




```

# high availability in a Docker swarm cluster

* High availability in Docker swarm involves multiple managers.
  * Ensures cluster functionality despite manager issues.
* Docker uses Raft consensus algorithm for consistent state.
  * More managers increase fault tolerance but require more communication.
* Quorum means majority (>50%) of swarm managers.
  * Maintaining quorum is essential for making changes.
* Examples illustrate quorum's importance during outages.
* Odd number of swarm managers recommended for fault tolerance.
* Availability zones should have at least 3 zones for redundancy.
  * Distribute swarm managers to maintain quorum if a zone fails.


```bash

docker node ls
# shows nodes

# RAFT Consensus Algorithm
## how many managers can we loose, and still keep afloat ?
## too many would lead to too much network work to keep track of them

# Quarum ... always more than 1/2
# no quarum, then no changes can be made to the swarm 
# always have an odd number of swarm managers
## 3sm ... can loose 1
## 5sm ,,, can loose 2
## 7sm ,,, can loose 3
## 9sm ,,, can loose 4

# Avaliability Zone Distrobution
# 3 | 1-1-1
# 5 | 2-2-1
# 7 | 3-2-2
# 9 | 3-3-3
# spread out the SM nodes, different distrobution centers or different locations

```

# Introduction to Docker Services

* Docker swarm services run applications in a cluster.
  * Defined by a set of replica tasks distributed across nodes.
* Tasks are containers; services are collections of tasks.
* Create services with `docker service create`.
  * Similar to `docker run` with additional options.
* Expose ports, use templates for dynamic values.
* Manage services with commands like `docker service ls`.
  * Inspect services with `docker service inspect`.
  * Update services with `docker service update`.
  * Scale services with `docker service scale`.
* Replicated services use a fixed number of replicas.
* Global services create one task on each node.


```bash

dcoker service create nginx
# creates a service on the nginx image

docker service create --name nginx --replicas 3 -p 8080:80 nginx
# creates 3 replicas, with an exposed port

curl localhost:8080
# shows it's running

###

docker service create --name node-hostname --env NODE_HOSTNAME="{{.NODE.Hostname}}" --replicas 3 nginx 
# creates a service with 3 replicas and they have the hostname as an environment variable

docker ps
# shows containers

docker exec <container id> printenv
# prints the environment variables

###

docker service ls
# list services


docker service ps nginx
# shows us the tasks of the service

docker service inspect nginx
# json output of the service

docker service inspect --pretty nginx
# shows it pretty 

docker service update --replicas 2 nginx
# update replicas

docker service rm nginx
# removes the service ... everything under the service will be removed also

docker service create --name nginx --mode global nginx
# creates 1 service on each node of the cluster 

docker service rm nginx 
# removes the service

###

docker service update --replicas 3 
# scales the to 3 replicas


docker service scale nginx=4
# scales up the service 


docker service ps nginx
# shows the replicas




```

# Using docker inspect

* Docker inspect provides detailed info about Docker objects.
  * Works for containers, images, and services.
* Use `docker inspect <ID>` or `docker inspect <NAME>`.
* Object-specific versions available:
  * `docker container inspect <NAME/ID>`
  * `docker service inspect <NAME/ID>`
* Some versions support `--pretty` flag for readable output.
* `--format` flag allows custom output formatting.
  * Utilizes Go templates for specific field extraction.
* Useful for troubleshooting and managing Docker objects.


```bash 

docker run -d --name nginx nginx

docker inspect <container id>

docker container ls
# list containers
docker inspect <container id>

docker image ls
# docker image list
docker inspect <image id>

docker service create --name nginx-csv nginx 

docker inspect <service id>

docker container inspect <id>

docker service inspect <id>

docker service inspect --pretty <id>
# more readable 

docker container inspect --pretty <id>
# doesn't work 

###

docker service inspect nginx-svc
docker service inspect --format='{{.ID}}' nginx-svc
# gives ID only 



```

# docker compose

* Docker Compose manages multi-container apps.
* Multi-container apps run different images.
* Define apps in a declarative docker-compose.yml file.
* Services specify containers and options.
* Use `docker-compose up` to run the app.
* `docker-compose down` stops and removes the app.
* Useful for development and testing.


```bash

sudo curl -L "https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(una
sudo chmod +x /usr/local/bin/docker-compose
docker-compose version

mkdir nginx-compose
cd nginx-compose
vi docker-compose.yml
#
# Docker Compose version 3
version: '3'
# Define services for our multi-container application
services:
  # Web service using the official Nginx image
  web:
    image: nginx
    ports:
      - "8080:80"  # Map host port 8080 to container port 80
  # Redis service using the official Redis image (alpine version)
  redis:
    image: redis:alpine
#

docker-compose up -d

docker-compose ps
docker-compose down



```


# docker stacks

* Docker Stacks Overview
  * Manage multi-container apps
  * Interact within a Docker Swarm
* Similar to Docker Compose
  * Run multi-container apps on a single host
  * Stacks run as services in a Docker Swarm
* Stack Creation and Management
  * Use docker stack deploy
  * List stacks with docker stack ls
  * View service tasks with docker stack ps
  * List services in a stack with docker stack services
  * Access service logs with docker service logs
  * Removing a stack with docker stack rm
* Configuring Stacks
  * Define services and replicas in a .yml file
  * Set environment variables for services
  * Publish ports to expose services
  * Enable communication between services in the stack
* Scaling Services
  * Scale services using replicas
  * Easily adjust the number of service instances
* Deleting Stacks
  * Remove a stack and its associated resources
* Exploring Docker Stack Features
  * Numerous features beyond this overview
  * Consult Docker documentation for details


```bash

vi simple-stack.yml
#
version: '3'
service:
    web:
        image: nginx
    busybox:
        image: radial/busyboxplus:curl
        command: /bin/sh -c "while true; do echo hello!; sleep 10; done"
#

docker stack deploy -c simple-stack.yml simple
# creates a stack

docker stack ls
# lists the stacks

docker stack ps simple
# shows the tasks

docker stack services simple
# shows the services

docker service logs <name of service>
# shows the logs from the service

vi simple-stack.yml
#
version: '3'
service:
    web:
        image: nginx
    busybox:
        image: radial/busyboxplus:curl
        command: /bin/sh -c "while true; do echo $$MESSAGE; sleep 10; done"
        environment:
        - MESSAGE=Hello!
#
docker stack deploy -c simple-stack.yml simple

vi simple-stack.yml
#
version: '3'
service:
    web:
        image: nginx
        ports:
        - "8080:80"
    busybox:
        image: radial/busyboxplus:curl
        command: /bin/sh -c "while true; do echo $$MESSAGE; sleep 10; done"
        environment:
        - MESSAGE=Hello!
#
docker stack deploy -c simple-stack.yml simple

vi simple-stack.yml
#
version: '3'
service:
    web:
        image: nginx
        ports:
        - "8080:80"
    busybox:
        image: radial/busyboxplus:curl
        command: /bin/sh -c "while true; do echo $$MESSAGE; curl web:80; sleep 10; done"
        environment:
        - MESSAGE=Hello!
#
docker stack deploy -c simple-stack.yml simple

curl localhost:8080
# test 

docker service logs simple_busybox
# shows the logs for busybox


vi simple-stack.yml
#
version: '3'
service:
    web:
        image: nginx
        ports:
        - "8080:80"
        deploy:
            replicas: 3
    busybox:
        image: radial/busyboxplus:curl
        command: /bin/sh -c "while true; do echo $$MESSAGE; curl web:80; sleep 10; done"
        environment:
        - MESSAGE=Hello!
#
docker stack deploy -c simple-stack.yml simple

docker stack ps simple
# shows the tasks

docker stack rm simple
# delete/remove stack



```

# Node labels

* Node Labels in Docker Swarm
  * Metadata for nodes
  * Control task execution
* Use Case Example
  * Managing tasks across zones
* Adding Labels to Nodes
  * `docker node update --label-add`
  * Key-value pairs for labels
* Viewing Node Labels
  * `docker node inspect`
  * Lists existing labels
* Controlling Task Placement
  * `--constraint` for specific labels
  * `==` for label equality
  * `!=` for label inequality
  * Constraints with `docker service create`
* Evenly Balancing Tasks
  * `--placement-pref` for task distribution
  * Spreading tasks across label values


```bash

docker node ls
# lists nodes

docker node update  --label-add availability_zone=east <node0 name>
docker node update  --label-add availability_zone=west <node1 name>

docker node inspect --pretty <node0 name>
docker node inspect --pretty <node1 name>

###

docker service create --name nginx-east --constraint node.label.availability_zone==east --replicas 3 nginx
docker service ps nginx-east

docker service create --name nginx-west --constraint node.label.availability_zone!=east --replicas 3 nginx
docker service ps nginx-west

###

docker service create --name nginx-spread --placement-pref spread=node.label.availability_zone --replicas 3 nginx

docker service ps nginx-spread
# shows the services 

docker service rm nginx-spread
#removes the services

```


# Lab -- Building services in Docker


Building Services in Docker
===========================

Introduction
------------

Services are the most basic and straightforward way to run containers using a Docker swarm. They allow you to execute multiple replica containers across all nodes in the Swarm cluster.

In this lab, you will have the opportunity to work with Docker services. You will practice scaling services by changing the number of replicas for an existing service. You will also have the opportunity to create a new service and run it in the cluster.

Solution
--------

1.  Begin by logging in to the lab server using the credentials provided on the hands-on lab page:

`ssh cloud_user@PUBLIC_IP_ADDRESS`

ssh cloud_user@3.238.93.105
&5!3Z_nw

ssh cloud_user@34.201.9.203
&5!3Z_nw

ssh cloud_user@44.200.229.18
&5!3Z_nw


### Scale the `products-fruit` service to 5 replicas

1.  Scale the service.

`docker service update --replicas 5 products-fruit`

You can also do it this way (both do the same thing):

`docker service scale products-fruit=5`

### Create the `products-vegetables` service

1.  Create the `products-vegetables` service.

`docker service create --name products-vegetables -p 8081:80 --replicas 3 linuxacademycontent/vegetable-service:1.0.0`

1.  Verify that the service is working.

`curl localhost:8081`

You should see some JSON data containing a list of vegetables.

Conclusion
----------

Congratulations — you've completed this hands-on lab!


# another Lab

Building a Docker Application Stack
===================================

Introduction
------------

Stacks are one of the most powerful orchestration features available in Docker Swarm. They allow you to easily manage complex applications consisting of multiple interdependent components running in separate containers.

In this lab, you will have the opportunity to work with Docker stacks by building a multi-component application as a Docker stack. You will also learn how to manage existing stacks by scaling a stack's services after it has already been deployed. This will give you some hands-on insight into Docker stacks.

Solution
--------

1.  Begin by logging in to the lab server using the credentials provided on the hands-on lab page:

`ssh cloud_user@PUBLIC_IP_ADDRESS`

ssh cloud_user@44.208.164.125
^tbM6^rt

### Build and deploy the application stack

1.  Create an empty project directory with a Docker compose YAML file inside.

`
cd ~/
mkdir produce
cd produce
vi produce.yml
`

1.  Build a stack definition in `produce.yml` to meet the provided specifications.

`
version: '3'
services:
   fruit:
     image: linuxacademycontent/fruit-service:1.0.1
   vegetables:
     image: linuxacademycontent/vegetable-service:1.0.0
   all_products:
     image: linuxacademycontent/all-products:1.0.0
     ports:
     - "8080:80"
     environment:
     - FRUIT_HOST=fruit
     - FRUIT_PORT=80
     - VEGETABLE_HOST=vegetables
     - VEGETABLE_PORT=80
`

1.  Deploy the stack using the compose file.

`docker stack deploy -c produce.yml produce`

1.  Verify that the stack is working.

`curl localhost:8080`

Note that after deploying, it may take a few moments for the stack to become responsive. You can check the status of the services with `docker stack services produce`. Once the services are up and running, you should get some JSON data containing a combined list of fruits and vegetables.

### Scale the _Fruit_ and _Vegetable_ services in the stack

1.  Set the number of replicas to `3` for the _Fruit_ and _Vegetable_ services in the compose file.

`vi produce.yml`

`
vi produce.yml
version: '3'
services:
   fruit:
     image: linuxacademycontent/fruit-service:1.0.1
     deploy:
       replicas: 3
   vegetables:
     image: linuxacademycontent/vegetable-service:1.0.0
     deploy:
       replicas: 3
   all_products:
     image: linuxacademycontent/all-products:1.0.0
     ports:
     - "8080:80"
     environment:
     - FRUIT_HOST=fruit
     - FRUIT_PORT=80
     - VEGETABLE_HOST=vegetables
     - VEGETABLE_PORT=80
     `

1.  Redeploy the stack using the compose file.

`docker stack deploy -c produce.yml produce`

1.  Verify that the stack is still working.

`curl localhost:8080`

You should get some JSON data containing a combined list of fruits and vegetables.

Use `docker stack services produce` to see that the number of replicas for the _Fruit_ and _Vegetable_ services is now 3.

Conclusion
----------

Congratulations — you've completed this hands-on lab!


# Storage and Volumns

* Docker Storage Concepts
  * Storage drivers (graph drivers)
  * Depends on OS and config
* Major Storage Drivers
  * Overlay2 (default for Ubuntu, CentOS, Red Hat)
  * Aufs (for older Ubuntu versions)
  * Devicemapper (for older CentOS)
* Storage Models
  * Filesystem Storage
    * Efficient memory usage
    * Less efficient for frequent writes
  * Block Storage
    * Requires separate block storage device
    * Efficient with write-heavy workloads
  * Object Storage
    * External to containers
    * Requires application integration
* Layered File System
  * Composed of multiple layers
  * Located on Docker host
  * Use `docker inspect` to find layer data


```bash


docker run --name storage_nginx nginx 

docker container inspect storage_nginx

sudo -i 
cd /var/lib/.../
ls
# directories

###

docker image inspect nginx


```

# configur devicemapper

* Configuring DeviceMapper for Docker
  * Supported by CentOS 7 and earlier
  * Customize using daemon config file
* DeviceMapper Modes
  * Loop-LVM (Default)
    * Uses loopback mechanism
    * Not recommended for production
  * Direct-LVM (Recommended)
    * Requires additional storage device
    * More efficient for production
* Steps to Configure Direct-LVM
  1. Add an additional storage device (e.g., /dev/xvdb)
  2. Stop and disable Docker
  3. Delete existing Docker data
  4. Edit daemon.json to configure DeviceMapper
  5. Enable and start Docker
* Verify Configuration
  * Check Docker info for storage driver and mode
  * Test by running a container (e.g., `docker run hello-world`)


```bash

docker info

sudo systemctl disable docker
sudo systemctl stop docker
sudo rm -rf /var/lib/docker
sudo vi /etc/docker/daemon.json
#
{
"storage-driver": "devicemapper",
"storage-opts": [
  "dm.directlvm_device=/dev/nvme1n1",
  "dm.thinp_percent=95",
  "dm.thinp_metapercent=1",
  "dm.thinp_autoextend_threshold=80",
  "dm.thinp_autoextend_percent=20",
  "dm.directlvm_device_force=true"
]
#



```